\section{Homework 10}
Sundry: I worked alone.

\begin{enumerate}
    \item \begin{enumerate}
        \item The sample space is $\{G, BG, BBG, BBB\}$, with probabilities:
        \[
            \P[G] = \frac{1}{2}, \quad
            \P[BG] = \frac{1}{4}, \quad
            \P[BBG] = \frac{1}{8}, \quad
            \P[BBB] = \frac{1}{8}.
        \]
        
        \item \[\begin{tabular}{c|c|c|c}
                  & C = 1 & C = 2 & C = 3 \\
            \hline
            G = 0 & 0 & 0 & 0.125 \\
            \hline
            G = 1 & 0.5 & 0.25 & 0.125
        \end{tabular}\]
        
        \item \[\begin{tabular}{c|c}
            $\P[G = 0]$ & 0.125 \\
            \hline
            $\P[G = 1]$ & 0.875
        \end{tabular}\]

        \[\begin{tabular}{c|c|c}
        $\P[C = 1]$ & $\P[C = 2]$ & $\P[C = 3]$ \\
        \hline
        0.5 & 0.25 & 0.25
        \end{tabular}\]
        
        \item No, they are not independent. If $C = 1$ or $C = 2$, then we know for sure that the Brown's had a girl, so $G = 1$ with probability 1. If $C = 3$, then there's a 0.5 chance they had a girl and a 0.5 chance that they did not have a girl, so $G = 1$ with probability $0.5 \neq 1$.
        
        \item 
        \begin{align*}
        \E[G] &= 0.125 \cdot 0 + 0.875 \cdot 1 = 0.875 \text{ girls} \\
        \E[C] &= 0.5 \cdot 1 + 0.25 \cdot 2 + 0.25 \cdot 3 = 1.75 \text{ children}.
        \end{align*}
    \end{enumerate}
    
    \item \begin{enumerate} 
        \item Let $I_k$ be the indicator variable for each of the $1 \leq k \leq n$ customers. That is,
    \[ I_k =
    \begin{cases}
        1 \text{ if customer receives own package unopened}, \\
        0 \text{ else}.
    \end{cases}
    \]
    Then by linearity of expectation, we get
    \begin{align*}
    \E\left[\sum\limits_{k = 1}^n I_k\right] &= \sum\limits_{k = 1}^n \P[I_k = 1] \\
        &= n \left( \frac{1}{n} \cdot \frac{1}{2} \right) \\
        &= \frac{1}{2}
    \end{align*}
    
    \item Using linearity, we know that
    \begin{align*}
        var(X) &= \E[X^2] - \E[X]^2 \\
            &= \E[(I_1 + \dots + I_n)^2] - \left(\frac{1}{2}\right)^2 \\
            &= \sum\limits_{k = 1}^n\E[I_k^2] + \sum_{\substack{1 \leq i \leq n \\ 1 \leq j \leq n \\ j \neq i}}\E[I_iI_j] - \frac{1}{4} \\
            &= \sum\limits_{k = 1}^n\P[I_k = 1] + \sum_{\substack{1 \leq i \leq n \\ 1 \leq j \leq n \\ j \neq i}}\P[I_i = 1, I_j = 1] - \frac{1}{4} \\
            &= \frac{1}{2} + \binom{n}{2}\left(\frac{1}{4}\right)\frac{1}{n(n-1)} - \frac{1}{4} \\
            &= \frac{3}{8}
    \end{align*}
    \end{enumerate}
    
    \item \begin{enumerate}
        \item \begin{enumerate} \item We have 
            \begin{align*}
                cov(X + Y, X - Y) &= \E[X^2 - Y^2] - \E[X + Y]\E[X - Y] \\
                    &= \E[X^2] - \E[Y^2] - (\E[X^2] - \E[Y^2]) \\
                    &= var(X) - var(Y) \\
                    &= 0
            \end{align*}
            
            \item Suppose $X - Y = 0$, then $\P[X + Y = 2] = \frac{1}{6}$, since we must have $X = 1, Y = 1$ out of $X = k, Y = k$ for $1 \leq k \leq 6$ possibilities. However, $\P[X + Y = 2] = \frac{1}{36}$ since there are 36 total sample points and one way to roll two ones. Thus it becomes clear that $X + Y$ and $X - Y$ are not independent, as $\P[X + Y = 2 | X - Y = 0] \neq \P[X + Y = 2]$.
        \end{enumerate}
        
        \item Yes. For all possible values $x$ of the random variables $X$, we know that $0 = var(X) = \E[(X - \mu)^2] = \sum\limits_x(x - \mu)^2\P[(X - \mu)^2 = (x - \mu)^2] \geq 0$, since squares are all nonnegative and probabilities are all nonnegative as well. Equality holds when $x = \mu$ for all $x$, thus $X$ must be constant.
        
        \item No. By linearity, we have
        \begin{align*}
            var(cX) &= \E[(cX)^2] - \E[cX]^2 \\
                &= c^2\E[X]^2 - (c\E[X])^2 \\
                &= c^2(\E[X]^2 - \E[X]^2) \\
                &= c^2var(X).
        \end{align*}
        
        \item No. Since $\f{corr}(A, B) = 0$, we know that $\f{cov}(A, B) = 0$, but this does not necessarily imply that $A$ and $B$ are independent. For a counterexample, consider the random variable $X$ which has a uniform probability in the set $\{-1, 0, 1\}$. We can see that $\f{corr}(X, X^2) = 0$, however it is clear that $X^2$ depends on $X$, and they are thus not independent.
        
        \item Yes. Since $\f{corr}(X, Y) = 0$, we know that $\f{cov}(X, Y) = 0$, and so $\E[XY] - \E[X]\E[Y] = 0$. Then we have
        \begin{align*}
            \f{var}(X + Y) &= \E[(X + Y)^2] - \E[X + Y]^2 \\
                &= \E[X^2] + 2\E[XY] + \E[Y^2] - \E[X]^2 - 2\E[X]\E[Y] - \E[Y]^2 \\
                &= (\E[X^2] - \E[X]^2) + (\E[Y^2] - \E[Y]^2) \\
                &= \f{var}(X) + \f{var}(Y).
        \end{align*}
        
        \item Yes. For every sample point $\omega$, suppose $X = x$ and $Y = y$, then the value $\omega$ contributes to the total expected value is $\frac{xy}{|\Omega|}$ regardless of whether $x < y$ or $x \geq y$. If we sum up all the values at each sample point, then it follows that $\E[\f{min}(X, Y)\f{max}(X, Y)] = \E[XY]$.
        
        \item Yes. By the same argument as above, we may deduce that multiplication of $X$ and $Y$ is commutative. That is, $\f{cov}(X, Y) = \E[XY] - \E[X]\E[Y] = \f{cov}(Y, X)$. And so we have $\f{Corr}(\f{max}(X, Y), \f{min}(X, Y)) = \f{Corr}(X, Y)$.
    \end{enumerate}
\end{enumerate}