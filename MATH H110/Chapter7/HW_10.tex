\textbf{7.1 Solution.} (Universal Property of Quotient Algebras) Let $T: A \to B$ be an algebra homomorphism between algebras $A$ and $B$. Furthermore, suppose $I$ is an ideal of $A$ such that $I \subseteq ker(T)$. Also let $\pi_I: A \to A / I$ be the quotient map. Then there exists a unique algebra homomorphism $\hat{T}: A / I \to B$ such that $T = \hat{T} \circ \pi_I$.

The proof is essentially the same as the proof for universal property of vector quotient spaces. The only extra thing we need to show is that $\hat{T}$ preserves multiplicity (Remember, we define $\hat{T}([x]) = T(x)$ for every $[x] \in I$). Suppose we have cosets $[x], [y] \in A / I$. Then we have $\hat{T}([x][y]) = \hat{T}([xy]) = T(xy) = T(x)T(y) = \hat{T}([x])\hat{T}([y])$ since $T$ is an algebra homomorphism. It follows that $\hat{T}$ is an algebra homomorphism as well. Uniqueness follows in the same fashion as uniqueness followed for vector space quotients.

\textbf{7.2 Solution.} (a) Apply the forgetful functor $F: Alg_{\F} \to Vec_{\F}$ to $A / I$. That is, we get $F(A / I) = F(A) / F(I)$, preserving vector space structures but forgetting the structure induced by algebra multiplicity. Since $F(A)$ and $F(I)$ are both vector spaces, it follows that $F(A) / F(I)$ is a vector space, and so is $F(A / I)$. Since remembering structure cannot make a vector space \textit{not} a vector space, we deduce that $A / I$ is also a vector space.

(b) Suppose we have $[a] \in A / I$ and $b, b' \in A$ such that $b \equiv b'$. Then we have $b - b' \in I$, and so $a(b - b') = ab - ab' \in I$. It follows that $ab \equiv ab'$, or $[ab] = [ab']$. Similarly, if we have an $a' \in A$ such that $a \equiv a'$, we can extend this to $ab \equiv a'b'$ or $[ab] = [a'b']$. Thus, the rule $[a][b] = [ab]$ is well-defined, i.e. independent of choice of coset.

(c) We already know $A / I$ is a well-defined vector space under coset addition and scalar multiplication. Now, we need to show it is closed under vector multiplication. Suppose we have $[a], [b] \in A / I$. Then by the definition from part (b), we can take coset multiplication to be $[a][b] := [ab] \in A / I$. We know that this is well-defined and therefore $A / I$ is an algebra.

(d) If $A$ is unital, then for every $a \in A$, we have the unit of multiplication $1 \in A$. Then the coset $[1]$ is the unit in $A / I$, since $[1][a] = [a] = [a][1]$ for every $[a] \in A / I$. Thus $A / I$ is unital as well.

(e) If $A$ is associative, then for every $a, b, c \in A$, we have $(ab)c = a(bc)$. It follows that for every $[a], [b], [c] \in A / I$, we have $([a][b])[c] = [ab][c] = [(ab)c] = [abc] = [a(bc)] = [a][bc] = [a]([b][c])$. Therefore $A / I$ is an associative algebra as well.

\textbf{7.3 Solution.} We have that $(I^{\vee} \cap T^k(V))$ is the collection of tensors in $I^{\vee}$ of homogeneous degree $k$. It is easy to see that $(I^{\vee} \cap T^k(V)) \cap (I^{\vee} \cap T^l(V)) = \{\}$ for all distinct $k, l \in \N$. Furthermore, $\bigoplus_{k \in \N}(I^{\vee} \cap T^k(V))$ spans $I^{\vee}$, so it is a direct sum decomposition of $I^{\vee}$. Thus $I^{\vee}$ is homogeneous in $T(V)$.

\textbf{7.4 Solution.} Yes. Suppose $(v_1, \dots, v_n)$ is a basis of $\R^n$. Then let $V$ be the vector space generated by the linear forms $\frac{\partial}{\partial v_i}$ where $1 \leq i \leq n$. In particular, $V$ is a subspace of the algebraic dual of $\R^n$. We treat vector addition and scalar multiplication the same as with functions in the function space $\mathcal{F}(\R^n, \R^n)$. It follows that $S(V) = Diff(\R^n)$.

\textbf{7.5 Solution.} $S$ takes $Id_V: V \to V$ to $S(Id_V): S(V) \to S(V)$ given by $x_1 \vee \dots \vee x_k \mapsto x_1 \vee \dots \vee x_k$, which is just the identity map, so $S(Id_V) = Id_{S(V)}$. Furthermore, $S$ preserves compositions since functions are applied term-wise to the $k$-vector, i.e. $S(F \circ G)$ and $S(F) \circ S(G)$ are both the map given by $x_1 \vee \dots \vee x_k \mapsto G(F(x_1)) \vee \dots \vee G(F(x_k))$. It is easy to deduce that $S$ is covariant, as it preserve the direction of the arrows.

\textbf{7.6 Solution.} $S(V)$ is the same as $T(V)$ except cosets stay the same under adjacent exchanging of terms. Then $S^1(V)$ is just $T^1(V)$, which is just $V$. By the universal property of inclusion into direct sums, we get that $V$ is naturally included into $S^0(V) \oplus S^1(V) \oplus S^2(V) \oplus \dots = S^0(V) \oplus V \oplus S^2(V) \oplus \dots$. This $\iota_V$ map coincides with the same map in 7.1.6 since every $v \in V$, is sent to $v \in T^1(V)$ via $\tau_V$, which is then sent to $[v] \in S(V)$ via $\pi_V$.

\textbf{7.7 Solution.} We construct the free space $\F\< (v_{i_1}, \dots, v_{i_k}) \>$ with basis $\delta_{(v_{i_1}, \dots, v_{i_k})}$ for $i_1 \leq \dots \leq i_k \in I$. This free space satisfies the universal property for multilinear maps to the symmetric algebra, which is unique up to isomorphism. Therefore, there exists an isomorphism $\F\< (v_{i_1}, \dots, v_{i_k}) \> \cong S^k(V)$, and since $\delta_{(v_{i_1}, \dots, v_{i_k})}$ is a basis of the former, we know that $(v_{i_1} \vee \dots \vee v_{i_k})$ is a basis of $S^k(V)$ by definition.

\textbf{7.8 Solution.} First, we know that since $T^0(V) \oplus T^1(V) \oplus \dots$ is a direct sum decomposition of $T(V)$, any set of tensors $(\tau_0, \tau_1, \dots)$ is linearly independent where $\tau_i \in T^i(V)$. It follows that any set of tensors $(\sigma_0, \sigma_1, \dots)$ is linearly independent where $\sigma_i \in I^{\wedge} \cap T^i(V)$. It is easy to see that the codiagonal map is surjective, since each homogeneous component $I^{\wedge} \cap T^i(V)$ contains all the tensors of degree $i$ generated by $u \otimes v + v \otimes u$.

\textbf{7.9 Solution.} To show that the two ideals are the same, we show that we can create each tensor in the first ideal with tensors of the second and vice versa. For every $u, v \in V$, we can form the tensor in the latter ideal $(u + v) \otimes (u + v) - u \otimes u - v \otimes v = (u + v) \otimes (v + u) = u \otimes v + v \otimes u$, which is the generating form of the former ideal. Now, for every $u \in V$, we can form the tensor in the first ideal $\frac{1}{2}(u \otimes u + u \otimes u) = u \otimes u$, which is the generating form of the latter ideal. Therefore, the two ideals are indeed the same.

\textbf{7.10 Solution.} Since $\F$ and $V$ are naturally included in the tensor algebra $T(V)$, and the inclusion map $\iota_V: V \to \Lambda(V)$ is given by $\pi_{I^{\wedge}} \circ \tau_V$ which are both natural since universal properties are natural, we can deduce that the inclusion map is natural as well. Thus $\F$ and $V$ are naturally included in $\Lambda(V)$.

\textbf{7.11 Solution.} By Exercise 7.9, we know that since $u \otimes u$ is in the quotient, $[u \otimes u] = u \wedge u = 0$, the zero vector. Thus $v_1 \wedge \dots \wedge u \wedge u \wedge \dots \wedge v_k = 0$ as well.

\textbf{7.12 Solution.} We have
\begin{align*}
    (a_1e_1 + a_2e_2 + a_3e_3) \wedge (b_1e_1 + b_2e_2 + b_3e_3) &= a_1b_1(e_1 \wedge e_1) + a_2b_2(e_2 \wedge e_2) + a_3b_3(e_3 \wedge e_3) \\
    &+ a_1b_2(e_1 \wedge e_2) + a_1b_3(e_1 \wedge e_3) + a_2b_1(e_2 \wedge e_1) \\
    &+ a_2b_3(e_2 \wedge e_3) + a_3b_1(e_3 \wedge b_1) + a_3b_2(e_3 \wedge b_2) \\
    &= a_1b_2(e_1 \wedge e_2) + a_1b_3(e_1 \wedge e_3) + a_2b_1(e_2 \wedge e_1) \\
    &+ a_2b_3(e_2 \wedge e_3) + a_3b_1(e_3 \wedge b_1) + a_3b_2(e_3 \wedge b_2).
\end{align*}
This expression is similar to the definition of cross product the first coordinate becomes $a_2b_3 - a_3b_2$, with the second and third coordinates defined similarly. Notice that these are the coefficients of the terms from above.

\textbf{7.13 Solution.} We have that $u \wedge v = e_1 \wedge e_2 \wedge e_1 \wedge e_3 = e_1 \wedge (e_2 + e_3)$. So $u = e_1$ and $v = e_2 + e_3$.

\textbf{7.14 Solution.} (a) We have
\begin{align*}
    \gamma \wedge \gamma &= ((e_1 \wedge e_2) + (e_3 \wedge e_4)) \wedge ((e_1 \wedge e_2) + (e_3 \wedge e_4)) \\
    &= (e_1 \wedge e_2) \wedge (e_3 \wedge e_4) + (e_3 \wedge e_4) \wedge (e_1 \wedge e_2) \\
    &= 2(e_1 \wedge e_2 \wedge e_3 \wedge e_4) \\
    &\neq 0.
\end{align*}

(b) Suppose we can write $\gamma = u_1 \wedge u_2 = (e_1 \wedge e_2) + (e_3 \wedge e_4)$. Since the $e_i$ are a basis of $\R^4$, we can write $u_1$ and $u_2$ as a unique linear combination of the $e_i$. That is,
\begin{align*}
    \left(\sum\limits_{i = 1}^4 a_ie_i\right) \wedge \left(\sum\limits_{i = 1}^4 b_ie_i\right) &= (e_1 \wedge e_2) + (e_3 \wedge e_4) \\
    \sum\limits_{i = 1}^4\sum\limits_{j = 1}^4 a_ib_j(e_i \wedge e_j) - (e_1 \wedge e_2) + (e_3 \wedge e_4) = 0
\end{align*}
Since $(e_1, e_2, e_3, e_4)$ is a linearly independent set, we must have that all the coefficients are equal to 0. This implies $a_1b_2 = 1$, $a_3b_4 = 1$, and all the other $a_ib_j = 0$. But this implies that at least one of $a_1$ and $b_4$ is 0, so at least one of $a_1b_2$ and $a_3b_4$ is 0 as well, a contradiction. Thus we cannot write $\gamma$ as a simple 2-vector.

(c) If we could write $(e_1 \otimes e_2) + (e_3 \otimes e_4)$ as a simple tensor, then we could also write $(e_1 \wedge e_2) + (e_3 \wedge e_4)$ as a simple 2-vector by applying the projection map. However, we can't write $(e_1 \wedge e_2) + (e_3 \wedge e_4)$ as a simple 2-vector, so therefore we cannot write $(e_1 \otimes e_2) + (e_3 \otimes e_4)$ as a simple tensor.

\textbf{7.15 Solution.} We simplify:
\begin{align*}
    (a(v_3 \^ v_4) + b(v_1 \^ v_3)) \^ (c(v_1 \^ v_2) + d(v_1 \^ v_4)) &= ac(v_3 \^ v_4 \^ v_1 \^ v_2) + 0 + 0 + 0 \\
    &= ac(v_1 \^ v_2 \^ v_3 \^ v_4).
\end{align*}
Since the latter 3 terms all have duplicate vectors, and thus evaluate to the zero 4-vector.

\textbf{7.16 Solution.} Note that if the terms of any $k$-vector are linearly dependent, then we can write one of the components as a linear combination of the others, split the $k$-vector into the components of the linear combination, and evaluate to zero since every part will have duplicate vectors. Therefore, if $k > dim(V)$, then we will necessarily have a non-linearly independent set, and $\Lambda_k(V) = 0$.

\textbf{7.17 Solution.} We will move the $v_i$ to the left of the $u_j$ one component at a time. To move the first term $v_1$ to the left of all the $u_j$'s, we must perform $p$ swapping operations. This will alternate the sign by a factor of $(-1)^p$. Now, $v_2$ becomes the first element in the list to the right of all the $u_j$'s. Inductively, we can see that for every $v_i$, we will multiply the expression by $(-1)^p$ in order to move it to the left of all the $u_j$'s. Since $1 \leq i \leq q$, there will be a resulting factor of $(-1)^{pq}$. That is, $(u_1 \^ \dots \^ u_p) \^ (v_1 \^ \dots \^ v_q) = (-1)^{pq}(v_1 \^ \dots \^ v_q) \^ (u_1 \^ \dots \^ u_p)$.

\textbf{7.18 Solution.} The first one simplifies to:
\begin{align*}
    (a(v_1 \^ v_3) + b(v_2 \^ v_4)) \^ (c(v_1 \^ v_3) + d(v_2 \^ v_4)) &= bc(v_2 \^ v_4 \^ v_1 \^ v_3) \\
    &\quad + ad(v_1 \^ v_3 \^ v_2 \^ v_4) \\
    &= (-bc - ad)(v_1 \^ v_2 \^ v_3 \^ v_4).
\end{align*}

The second one simplifies to:
\begin{align*}
    (av_1 + bv_4) \^ (c(v_1 \^ v_2 \^ v_3) + d(v_2 \^ v_3 \^ v_4)) &= ad(v_1 \^ v_2 \^ v_3 \^ v_4) \\
    &\quad + bc(v_4 \^ v_1 \^ v_2 \^ v_3) \\
    &= (ad - bc)(v_1 \^ v_2 \^ v_3 \^ v_4).
\end{align*}

\textbf{7.19 Solution.} The $\Lambda$ map preserves identities, since $\Lambda(Id_V)(v_1 \^ \dots \^ v_k) = v_1 \^ \dots \^ v_k$, which is just $Id_{\Lambda(V)}$. Furthermore, composition is preserved, as if we have two homomorphisms $F, G \in Hom(V, W)$, then we get $\Lambda(F \circ G)(v_1 \^ \dots \^ v_k) = F(G(v_1)) \^ \dots \^ F(G(v_k)) = \Lambda(F) \circ \Lambda(G)(v_1 \^ \dots \^ v_k)$. Finally, the direction of the arrows are preserved, so $\Lambda$ is a covariant functor.

\textbf{7.20 Solution.} Suppose we have 
\[
\sigma_1 = (u_1, \dots, u_i, u_{i+1}, \dots , u_k) \mapsto \sum\limits_{\sigma \in S_k}\f{sgn}(\sigma)f_1(u_{\sigma(1)}) \dots f_k(u_{\sigma(k)})
\]
Now, since $\sigma_2 = (u_1, \dots, u_{i+1}, u_i, \dots, u_k)$ is a permutation of the $u_i$ in itself, we know that the every permutation is still represented in the expansion of $M(\sigma_2)$. The only difference is the coefficient $\f{sgn}(\sigma)$ of each $\sigma \in S_k$. Since $\sigma_2$ differs by $\sigma_1$ by exactly one transposition (swapping $u_i$ and $u_{i+1}$), we deduce that each term in $M(\sigma_2$ correponds to the negative of each term in $M(\sigma_1)$. It follows that $M$ is an alternating multilinear functional.

\textbf{7.21 Solution.} We will show $(a) \implies (c) \implies (b) \implies (a)$.

($a \implies c$) Since $(v_1, \dots, v_k)$ are linearly independent, we know that those $k$ vectors are part of some basis of $V$. It follows by Thm. 7.2.16 that $(v_1 \^ \dots \^ v_k)$ is a basis vector of $\Lambda_k(V)$. Then we can construct the linear functional $f = \delta_{v_1 \^ \dots \^ v_k}$ that sends $v_1 \^ \dots \^ v_k$ to 1 and everything else to 0. We know that $f$ is part of some basis of $\Lambda_k(V)'$ since $v_1 \^ \dots \^ v_k$ is a part of some basis of $\Lambda_k(V)$, thus (a) implies (c).

($c \implies b$) Suppose for contradiction that $\alpha = 0 \in \Lambda_k(V)$. But then since homomorphism preserve additive identities, we know that $\<f | \alpha\> = 0$ for all $f \in \Lambda_k(V)'$, a contradiction. Thus (c) implies (b).

($b \implies a$) Suppose for contradiction that $(v_1, \dots, v_k)$ are linearly dependent. Then, WLOG we can write $v_k$ as a linear combination $\sum_{i = 1}^{k - 1}a_iv_i$. Then the wedge product $v_1 \^ \dots \^ v_k$ simplifies to $\sum_{i = 1}^{k - 1}a_i(v_1 \^ \dots \^ v_{k-1} \^ v_i) = 0$, since any wedge product with duplicate vectors evaluates to 0. Thus by contradiction (b) implies (a).

\textbf{7.22 Solution.} If $\alpha = v_1 \^ \dots \^ v_k$ is the zero vector in $\Lambda_k(V)$, then by Thm. 7.2.14, we have that $(v_1, \dots, v_k)$ are linearly dependent vectors in $V$. We know that linear dependence extends to supersets, so the set $(v_1, \dots, v_k, w)$ is linearly dependent for any $w \in V$. It follows by Thm. 7.2.14 that $v_1 \^ \dots \^ v_k \^ w$ is the zero vector.

\textbf{7.23 Solution.} We prove by induction on $k$. For $k = 1$, we get that $f_1 \neq 0$, so by 7.2.14 $f_1$ is linearly independent and so it cannot be the zero map. It follows that we can find a $u_1$ such that $f_1(u_1) \neq 0$.

Now for the inductive step. Suppose that for $k = n - 1$, 7.23 is true. Further suppose that $f_1 \^ \dots \^ f_{n-1} \^ f_n \neq 0$, so $(f_1, \dots, f_n)$ are linearly independent. By our inductive hypothesis, we know that since $(f_1, \dots, f_{n-1})$ are linearly independent, there exists $u_1, \dots, u_{n-1}$ such that 7.23 holds. Now, we can write $\<f_1 \^ \dots \^ f_n | u_1 \^ \dots \^ u_n\>$ as $\lambda_1f_1(u_n) + \lambda_2f_2(u_n) + \dots + \lambda_nf_n(u_n)$ for some scalars $\lambda_i$ dependent on choice of $u_i$. Note that $\lambda_n = \<f_1 \^ \dots \^ f_{n-1} | u_1 \^ \dots \^ u_{n-1}\>$, which by inductive hypothesis is nonzero. Thus by linear independence of the $f_i$, we know that the bra-ket evaluation of $k = n$ is also nonzero.

\textbf{7.24 Solution.} By Thm. 7.2.16, we know that if $(v_i)_{i \in I}$ is a basis of $V$, then $(v_{i_1} \^ \dots \^ v_{i_k})_{i_1 < i_2 \dots < i_k}$ is a basis of $\Lambda_k(V)$ and $(v^{i_1} \^ \dots \^ v^{i_k})_{i_1 < i_2 \dots < i_k}$ is a basis of $\Lambda_k(V')$. It follows that both vector spaces have dimension $\binom{n}{k}$, where $n = \f{dim}(V)$. We can construct a natural bijection between basis vectors $(v_{i_1} \^ \dots \^ v_{i_k}) \longleftrightarrow (v^{i_1} \^ \dots \^ v^{i_k})$ for every $i_1 < \dots < i_k \in I$.

\textbf{7.25 Solution.} 
\[\begin{tabular}{c|c|c}
$k$ & Basis of $\Lambda_k(\R^3)$ & Dimension \\
\hline
0 & (1) & 1 \\
1 & $(e_1, e_2, e_3, e_4)$ & 4 \\
2 & $(e_1 \^ e_2, e_1 \^ e_3, e_1 \^ e_4, e_2 \^ e_3, e_2 \^ e_4, e_3 \^ e_4)$ & 6 \\
3 & $(e_1 \^ e_2 \^ e_3, e_1 \^ e_2 \^ e_4, e_1 \^ e_3 \^ e_4, e_2 \^ e_3 \^ e_4)$ & 4 \\
4 & $(e_1 \^ e_2 \^ e_3 \^ e_4)$ & 1
\end{tabular}\]
The dimensions for each $\Lambda_k(\R^i)$ are the corresponding elements of the $i$th row of pascals triangle. This makes sense intuitively since we are essentially choosing $k$ basis vectors out of $n$ where $n$ is the dimension of $\R^n$.

\textbf{7.26 Solution.} We have that $v_1 = \frac{e_1 + e_2}{\sqrt{2}}$ and $v_2 = \frac{e_2 - e_1}{\sqrt{2}}$. Then we get
\begin{align*}
    v_1 \^ v_2 &= \left(\frac{e_1 + e_2}{\sqrt{2}}\right) \^ \left(\frac{e_2 - e_1}{\sqrt{2}}\right) \\
    &= \frac{1}{2}(e_1 \^ e_2 - e_2 \^ e_1) \\
    &= e_1 \^ e_2.
\end{align*}

