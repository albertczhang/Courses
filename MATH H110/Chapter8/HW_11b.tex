\textbf{8.1 Solution.} Let $(e_1, e_2)$ be our basis. Then we have $T(e_1) = ae_1 + ce_2$ and $T(e_2) = be_1 + de_2$. Then by the definition of determinant, we get
\begin{align*}
    \f{det}(T)e_1 \^ e_2 &= (ae_1 + ce_2) \^ (be_1 + de_2) \\
    &= (ae_1 + ce_2 - \frac{c}{d}(be_1 + de_2)) \^ (be_1 + de_2) \\
    &= (a - \frac{bc}{d})e_1 \^ (be_1 + de_2) \\
    &= (a - \frac{bc}{d})e_1 \^ (be_1 + de_2 - be_1) \\
    &= (a - \frac{bc}{d})e_1 \^ (de_2) \\
    &= (ad - bc)e_1 \^ e_2.
\end{align*}
It follows that $\det{T} = ad - bc$.

\textbf{8.2 Solution.} Suppose $(e_1, \dots, e_n)$ is our basis of the $n$-dimensional vector space $V$. Then we have a homomorphism/matrix such that $A(e_i)$ is the $i$th column vector of $A$, and we also have another homomorphism/matrix $D$ such that $D(e_k) = aA(e_k)$ for some value of $1 \leq k \leq n$ and $D(e_i) = A(e_i)$ everywhere else. It follows from the definition of determinant that
\begin{align*}
    \f{det}(D)e_1 \^ \dots \^ e_n &= D(e_1) \^ \dots \^ D(e_k) \^ \dots \^ D(e_n) \\
    &= A(e_1) \^ \dots \^ aA(e_k) \^ \dots \^ A(e_n) \\
    &= a(A(e_1) \^ \dots \^ A(e_n)) \\
    &= a\f{det}(A)e_1 \^ \dots \^ e_n.
\end{align*}
Therefore, we get that $\f{det}(D) = a\f{det}(A)$.

\textbf{8.3 Solution.} Since adding a scalar multiple of columns to other columns does not affect the determinant, we can add multiples of column 1 to every other column such that the first row is 0 everywhere except at $a_{11}$. Similarly, we can do this for each column vector, until the matrix has zeroes everywhere except along the diagonal. It follows that the determinant is the product of the diagonals, since the determinant stays the same under each column operation.

\textbf{8.4 Solution.} By the definition of direct sum of homomorphisms, we know that $C_i$ only acts on the vector in the $i$th coordinate of the input. That is, suppose we have some $v_1, \dots, v_k$ in the domains of $C_1, \dots, C_k$, respectively. Then we have
\begin{align*}
    A^j(v_1, \dots, v_k) &= (C_1 \oplus \dots \oplus C_k)^j(v_1, \dots, v_k) \\
    &= (C_1^j(v_1), \dots, C_k^j(v_k)) \\
    &= (C_1^j \oplus \dots \oplus C_k^j)(v_1, \dots, v_k).
\end{align*}
Since $A^j$ and $C_1^j \oplus \dots \oplus C_k^j$ agree on every point in their domain, we deduce that they are the same homomorphism.

\textbf{8.5 Solution.} Suppose $p(c) = a_0 + a_1c + \dots + a_nc^n$ for scalars $a_i \in \F$. Since $Id^n = Id$, we have
\begin{align*}
    \Tilde{p}(c\f{Id}) &= a_0\f{Id} + a_1(c\f{Id}) + \dots + a_n(c\f{Id})^n \\
    &= a_0\f{Id} + a_1c\f{Id} + \dots + a_nc^n\f{Id} \\
    &= (a_0 + a_1c + \dots + a_nc^n)\f{Id} \\
    &= p(c)\f{Id}.
\end{align*}

\textbf{8.6 Solution.} False. It is easy to see that composition is not preserved. Suppose we have endomorphisms $T_1, T_2 \in End(V)$ and $p \in \F^1[X]$ such that $p(x) = 1 + x$. Then 
\begin{align*}
    \Tilde{p}(T_1T_2) &= \f{Id} + (T_1T_2) \\
    &\neq \f{Id} + T_1 + T_2 + T_1T_2 \\
    &= (\f{Id} + T_1)\circ(\f{Id} + T_2) \\
    &= \Tilde{p}(T_1)\Tilde{p}(T_2).
\end{align*}
So, we can see that $\Tilde{p}$ is not a valid functor from $End(V)$ to itself.

\textbf{8.7 Solution.} Suppose for contradiction that $\lambda \in \R$ is a common root of $p$ and $p'$. Then we can write 
\[
p(x) = (x - \lambda)q(x)
\]
for some polynomial $q \in \R^{n-1}[X]$. Differentiating, we get 
\[
p'(x) = q(x) + (x - \lambda)q'(x),
\]
and since $\lambda$ is a root of $p'$ by assumption, we see that $\lambda$ must be a root of $q(x)$. But then $p$ has a duplicate root of $\lambda$, a contradiction. Thus $p$ and its derivative have no roots in common given that $p$ has distinct roots.
