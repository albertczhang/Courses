\section{Homework 11}

\subsection{Flipping Coins and Hypothesizing}
The likelihood function is
\[
L(y) = \frac{f_{Y|X}(y | 1)}{f_{Y|X}(y | 0)} = \frac{(1 - q)^{y - 1}q}{(1 - p)^{y - 1}p},
\]
which is strictly decreasing on $y$ since $q > p$. Thus the solution to this problem can be represented by $\hat{X} = 1\{y < y_0\} + \gamma 1\{y = y_0\}$. Given $\beta$, we maximize $\P[\hat{X} = 1 | X = 1]$ by solving
\begin{align*}
    \beta &= \P[\hat{X} = 1 | X = 0] \\
        &= 1 - (1 - p)^{y_0 - 1} + \gamma p(1 - p)^{y_0 - 1} \\
        &= 1 - (1 - \gamma p)(1 - p)^{y_0 - 1},
\end{align*}
which gives us 
\[
y_0 = 1 + \log_{1 - p}(1 - \beta) - \log_{1 - p}(1 - \gamma p),
\]
where we choose $\gamma \in [0, 1]$ so that $y_0$ is an integer. Thus the solution to our hypothesis testing problem is
\[
\hat{X} = \begin{cases}
1 &\quad\text{ if } y < y_0 \\
1 \text{  w.p. } \gamma &\quad\text{  if } y = y_0 \\
0 &\quad\text{ if } y > y_0.
\end{cases}\]

\subsection{BSC Hypothesis Testing}
Let $Y_1, \dots, Y_n$ be RV's denoting a 1 if bit $i$ was corrupted or a 0 if it was transmitted correctly. Also let $Y = \sum Y_i$, the Then since our likelihood function
\[
L(Y) = \frac{\epsilon'^y(1 - \epsilon')^{n - y}}{(0.1)^y(0.9)^{n - y}}
\]
is increasing on $y$, our solution becomes $\hat{X} = 1\{Y \geq n_0\}$. Applying CLT, we get
\begin{align*}
    \beta &= \P[\hat{X} = 1 | X = 0] \\
        &= \P[Y \geq n_0 | \epsilon = 0.1] \\
        &= \P[\Nor(0.1n, 0.09n) \geq n_0] \\
        &= \P\left[\Nor(0, 1) \geq \frac{n_0 - 0.1n}{0.3\sqrt{n}}\right].
\end{align*}
Given that $\beta = 0.05$, we get
\[
n_0 = 0.1n + 0.495\sqrt{n}.
\]
Thus, the solution is
\[
\hat{X} = \begin{cases}
1 &\quad\text{ if } Y \geq n_0, \\
0 &\quad\text{ if } Y < n_0.
\end{cases}
\]
Indeed, we see that our decision rule is independent of the choice of $\epsilon' > 0$.

\newcommand{\Hi}{\mathcal{H}}
\subsection{Projections}
(a) First, we show that $\Hi$ is a vector space. Let $X, Y \in \Hi$ be random variables with finite second moments. Then $\E[X^2] < \infty$ and $\E[Y^2] < \infty$. Then using Cauchy Schwarz, we have
\begin{align*}
    \E[(X + Y)^2] &= \E[X^2] + 2\E[XY] + \E[Y^2] \\
        & \leq \E[X^2] + 2\sqrt{\E[X^2]\E[Y^2]} + \E[Y^2] \\
        &< \infty.
\end{align*}
Thus $X + Y \in \Hi$, and so $\Hi$ is a vector space. Now, we verify that $\<\cdot,\cdot\>$ satisfies the axioms of an inner product space:
\begin{itemize}
    \item (symmetry) $\<X, Y\> = \E[XY] = \E[YX] = \<Y, X\>$.
    \item (linearity) $\<X + cZ, Y\> = \E[(X + cZ)Y] = \E[XY] + c\E[ZY] = \<X, Y\> + c\<Z, Y\>$.
    \item (positive definiteness) $\<X, X\> = \E[X^2] > 0$ whenever $X$ is not the $\mathbb{0}$ random variable.
\end{itemize}
Thus $\<X, Y\> := \E[XY]$ indeed fashions $\Hi$ into an inner product space.

(b) Let $u, v \in V$ and $c \in \R$. Then we have that
\begin{align*}
    ||(u + cv) - x||^2 &= ||(u + cv) - P_{u + cv}||^2 + 2\<(u + cv) - P_{u + cv}, P_{u + cv} - x\> + ||P_{u + cv} - x||^2 \\
    &= ||(u + cv) - P_{u + cv}||^2  + ||P_{u + cv} - x||^2 \\
    &\geq ||(u + cv) - P_{u + cv}||^2,
\end{align*} 
with equality if and only if $x = P_{u + cv}$. But we also have that
\begin{align*}
    ||(u + cv) - x||^2 &= ||(u + cv) - P_u - cP_{v}||^2 + 2\<(u + cv) - P_u - cP_{v}, P_{u} + cP_{v} - x\> + ||P_{u} + cP_{v} - x||^2,
\end{align*}
and since $U^\perp$ is a subspace, we have $(u - P_u) + (cv - cP_v) \in U^\perp$, and since $P_u, P_v, x \in U$, $P_u + cP_v - x \in U$, so that
\[
\<u + cv - P_u - cP_v, P_u + cP_v - x\> = 0,
\]
continuing, we get
\begin{align*}
    ||(u + cv) - x||^2 &= ||(u + cv) - P_u - cP_{v}||^2 + ||P_{u} + cP_{v} - x||^2 \\
        &\geq ||(u + cv) - P_u - cP_{v}||^2,
\end{align*}
with equality if and only if $x = P_u + cP_v$. But since $x$ was also the minimizer of $(u + cv)$, we see that $P_{u + cv} = P_u + cP_v$. Thus the projection map is linear.

(c) If $\{v_i\}_{i = 1}^n$ is an orthonormal basis, then each of the terms $\<y, v_i\>v_i$ gives the $v_i$-component of the projection in the subspace $U$. To see this, we write it out explicitly as
\[
\<y, v_i\>v_i = ||y||||v_i||\cos\theta_i = ||y||\cos\theta_i,
\]
where $\theta_i$ is the angle between $y$ and $v_i$. Then $||y||\cos\theta_i$ is the length of the projection of $y$ onto the line generated by $v_i$, which is precisely the $v_i$-component of the final projection $P_y$ we want.


\subsection{Exam Difficulties}
(a) After some calculations, we get the following
\begin{align*}
    \E[X] &= 25 \\
    \E[X^2] &= \frac{10000}{9} \\
    \E[\Theta] &= 50 \\
    \E[X\Theta] &= \frac{5000}{3},
\end{align*}
so that using the LLSE formula, we get
\begin{align*}
    L[\Theta | X] &= \E[\Theta] + \frac{\E[X\Theta] - \E[X]\E[\Theta]}{\E[X^2] - \E[X]^2}(X - \E[X]) \\
    &= 50 + \frac{6}{7}(X - 25) \\
    &= \frac{6}{7}X + \frac{200}{7}.
\end{align*}

(b) We have
\[
\MAP[\Theta | X] = \argmax_\theta f_{\Theta}(\theta)f_{X|\Theta}(X|\theta) = \argmax_\theta f_{X | \Theta}(X|\theta) = X.
\]


\subsection{Photodetector LLSE}
Let $X = \Theta + N$, where $\Theta$ is the product of a Bernoulli random variable with parameter $p$ and a Poisson with parameter $\lambda$. Then $X \sim Pois(\lambda)Bern(p) + Pois(\mu)$, so that we get the following calculations necessary for later:
\begin{align*}
    \E[X] &= \lambda p + \mu \\
    \E[X^2] &= p\lambda(\lambda + 1) + 2\lambda p\mu + \mu(\mu + 1) \\
    Cov(\Theta, X) &= Cov(\Theta, \Theta + N) = Cov(\Theta, \Theta) + Cov(\Theta, N) = Cov(\Theta, \Theta) = \lambda^2 p + \lambda p - \lambda^2 p^2.
\end{align*}
Using LLSE formula, we have
\begin{align*}
    L[\Theta | X] &= \E[\Theta] + \frac{\E[X\Theta] - \E[X]\E[\Theta]}{\E[X^2] - \E[X]^2}(X - \E[X]) \\
    &= \frac{\lambda^2 p + \lambda p - \lambda^2 p^2}{\lambda^2 p + \lambda p - \lambda^2 p^2 + \mu}(X - \lambda p - \mu) + \lambda p.
\end{align*}

% \begin{align*}
%     \E[X] &= \lambda p + \mu \\
%     \E[X^2] &= \lambda p(\lambda p + 1) + 2\lambda p\mu + \mu(\mu + 1) \\
%     Cov(\Theta, X) &= Cov(\Theta, \Theta + N) = Cov(\Theta, \Theta) + Cov(\Theta, N) = Cov(\Theta, \Theta) = \lambda p.
% \end{align*}
% Using LLSE formula, we have
% \begin{align*}
%     L[\Theta | X] &= \E[\Theta] + \frac{\E[X\Theta] - \E[X]\E[\Theta]}{\E[X^2] - \E[X]^2}(X - \E[X]) \\
%     &= \frac{\lambda p}{\lambda p + \mu}X.
% \end{align*}