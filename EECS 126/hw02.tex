\section{Homework 2}

\begin{enumerate}
    \item \textbf{Packet Routing}
    
    (a) Let $S$ be the random variable counting the number of arriving packets in the time interval $[0, 1]$, and let $X$ be the random variable counting the number of packets routed to $A$. We will show that $X$ is poisson distributed with parameter $\lambda p$. We compute the probability that $X = k$ as a summation based on the number of packets that arrive at the switch:
    \begin{align*}
        \P[X = k] &= \sum\limits_{i = k}^\infty \P[S = i]\binom{i}{k}(1 - p)^{i - k}p^k \\
            &= \sum\limits_{i = k}^\infty \frac{e^{-\lambda}\lambda^i}{i!}\binom{i}{k}(1 - p)^{i - k}p^k \\
            &= \frac{\lambda^k p^k e^{-\lambda}}{k!}\sum\limits_{j = 0}^\infty \frac{\lambda^j(1 - p)^j}{j!} \\
            &= \frac{\lambda^k p^k e^{-\lambda}e^{\lambda(1 - p)}}{k!} \\
            &= \frac{e^{-\lambda p}(\lambda p)^k}{k!} \sim \f{Poisson}(\lambda p).
    \end{align*}
    
    (b) Consider $\P[A = k | B = 0]$. This is equal to $\P[S = k]p^k = e^{-\lambda}\frac{\lambda^k}{k!}p^k$, that is, the probability that $k$ packets arrive at the switch and all $k$ of these packets are routed to $A$. However, by our result from part (a), $\P[A = k] = e^{-\lambda p}\frac{(\lambda p)^k}{k!}$. Thus, the number of packets routed to $A$ and the to $B$ cannot be independent since
    \[
    \P[A = k | B = 0] = e^{-\lambda}\frac{\lambda^k}{k!}p^k \neq e^{-\lambda p}\frac{(\lambda p)^k}{k!} = \P[A = k].
    \]
    
    \item \textbf{Compact Arrays}
    
    Note that $X = i - (\f{\# of 0's that appear before ith index})$. We can separate the second portion into i.i.d. indicator random variables $I_1, \dots, I_{i - 1}$, where $I_k$ indicates whether a 0 appears in the $k$th position or not. Then we have that
    \begin{align*}
    \E[X] &= \E[i - (\f{\# of 0's that appear before ith index})] \\
        &= i - \E[I_1 + \dots + I_{i - 1}] \\
        &= i - (i - 1)\E[I_1] \\
        &= i - (i - 1)\frac{1}{10} \\
        &= \frac{9i + 1}{10}.
    \end{align*}
    Now, we calculate the variance, using the same indicator variables and the property that they are mutually independent and identically distributed:
    \begin{align*}
        \f{Var}(X) &= \f{Var}(i - (I_1 + \dots + I_{i - 1})) \\
            &= \f{Var}(I_1 + \dots + I_{i - 1}) \\
            &= \f{Var}(I_1) + \dots + \f{Var}(I_{i - 1}) \\
            &= (i - 1)(\E[I_1^2] - \E[I_1]^2) \\
            &= (i - 1)(\frac{1}{10} - \frac{1}{100}) \\
            &= \frac{9i - 9}{100}.
    \end{align*}
    
    \item \textbf{Message Segmentation}
    
    (a) First, we note that the support of our PMF $f$ is $(q, r)$ where $0 \leq r < m$ whenever $q > 0$ and $0 < r < m$ whenever $q = 0$ (since $N$ is strictly positive and $N = mq + r$) for integers $q$ and $r$. Let $N$ be the random variable of number of bytes in the message. Now, we find our PMF to be
    \begin{align*}
        f(q, r) &= \P[Q = q, R = r] \\
            &= \P[N = mq + r] \\
            &= (1 - p)^{mq + r - 1}p
    \end{align*}
    where $q$ and $r$ are restricted to the support mentioned above.
    
    (b) First, we compute the marginal PMF of $Q$ when $q > 0$:
    \begin{align*}
        \P[Q = q] &= \sum\limits_{i = 0}^{m - 1}\P[Q = q, R = i] \\
            &= \sum\limits_{i = 0}^{m - 1}(1 - p)^{mq + i - 1}p \\
            &= (1 - p)^{mq - 1}p\sum\limits_{i = 0}^{m - 1}(1 - p)^i \\
            &= (1 - p)^{mq - 1}p\left(\frac{1 - (1 - p)^m}{1 - (1 - p)}\right) \\
            &= (1 - p)^{mq - 1} - (1 - p)^{mq + m - 1}.
    \end{align*}
    Now, if $q = 0$, then we replace the starting index 0 with 1 and instead get:
    \begin{align*}
        \P[Q = 0] &= \sum\limits_{i = 1}^{m - 1}\P[Q = 0, R = i] \\
            &= \sum\limits_{i = 1}^{m - 1}(1 - p)^{i - 1}p \\
            &=p\left(\frac{1 - (1 - p)^{m - 1}}{1 - (1 - p)}\right) \\
            &= 1 - (1 - p)^{m - 1}
    \end{align*}
    
    On the other hand, we compute the marginal PMF of $R$ for $R = r > 0$:
    \begin{align*}
        \P[R = r] &= \sum\limits_{i = 0}^\infty \P[Q = i, R = r] \\
            &= \sum\limits_{i = 0}^\infty(1 - p)^{mi + r - 1}p \\ 
            &= (1 - p)^{r - 1}p\sum\limits_{i = 0}^\infty((1 - p)^m)^i \\
            &= \frac{p(1 - p)^{r - 1}}{1 - (1 - p)^m}
    \end{align*}
    
    Now, we compute the marginal PMF of $R$ when $R = 0$:
    \begin{align*}
        \P[R = 0] &= \sum\limits_{i = 1}^\infty \P[Q = i, R = 0] \\
            &= \sum\limits_{i = 1}^\infty(1 - p)^{mi - 1}p \\
            &= \frac{p}{1 - p}\left(\frac{(1 - p)^m}{1 - (1 - p)^m}\right) \\
            &= \frac{p(1 - p)^{m - 1}}{1 - (1 - p)^m}.
    \end{align*}
    
    (c) Now that we are given $N > m$, $q$ is at least 1, and our support becomes nicer and with less casework. That is, our support now consists of points $(q, r)$ for integers $q$ and $r$ that satisfy $q > 0$ and $0 \leq r < m$.
    
    The marginal PMF of $Q$ stays the same:
     \begin{align*}
        \P[Q = q] &= \sum\limits_{i = 0}^{m - 1}\P[Q = q, R = i] \\
            &= (1 - p)^{mq - 1} - (1 - p)^{mq + m - 1}.
    \end{align*}
    
    We compute the updated marginal PMF of $R$:
    \begin{align*}
        \P[R = r] &= \sum\limits_{i = 1}^\infty\P[Q = i, R = r] \\
        &= \sum\limits_{i = 1}^\infty(1 - p)^{mi + r - 1}p \\
        &= p(1-p)^{m + r - 1}\sum\limits_{i = 0}^\infty(1 - p)^{mi} \\
        &= \frac{p(1 - p)^{m + r - 1}}{1 - (1 - p)^m}.
    \end{align*}
    
    \item \textbf{Almost Fixed Points of a Permutation}
    
    We can split $X$ into indicator variables $I_1, \dots, I_n$ where $I_k$ indicates whether or not $k$ is almost a fixed point of a given permutation. Then since the probability of a number being almost a fixed point is just $\frac{3}{n}$ (3 possible "almost fixed points", the two adjacent points and itself), we get the expectation using linearity
    \begin{align*}
        \E[X] &= \E[I_1 + \dots + I_n] \\
            &= \E[I_1] + \dots + \E[I_n] \\
            &= n \cdot \frac{3}{n} \\
            &= 3.
    \end{align*}
    
    For variance, we must split our calculations into cases with respect to $n$.
    
    Case 1 ($n = 1$):
    The only permutation sends 1 to 1. Thus the variance is 0.
    
    Case 2 ($n = 2$):
    There are two permutations, both of which produce 2 almost fixed points. Thus the variance is 0 again.
    
    Case 3 ($n = 3$):
    There are six permutations, all of which produce 3 almost fixed points. Thus the variance is 0 again.
    
    Case 4: ($n = 4$):
    Look at case 5 first. The only changes we need to make are to the counting of $\sum_{k \neq j}\E[I_kI_j]$. First, if $k$ and $j$ are adjacent, the expectation stays the same, namely
    \[
        \frac{14}{n - 1} = \frac{14}{3}.
    \]
    Now, there are 4 ways such that $k$ and $j$ are two indices apart. WLOG consider points 1, 2, 3, 4, where $k = 1, j = 3$. Then there are 7 ways to assign an ordered pair so that 1 and 3 are almost fixed points: (1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (4, 2), (4, 3). So the expectation for this case is 
    \[
        4 \cdot \frac{7}{n(n - 1)} = \frac{7}{3}.
    \]
    Adding everything up, we get
    \begin{align*}
        \f{Var}(X) &= \frac{14}{3} + \frac{7}{3} - 6 \\
            &= 1.
    \end{align*}
    
    
    Case 5 ($n > 4$): 
    We split our random variable $X$ into indicators $I_1, \dots, I_n$ where $I_k$ indicates whether or not $k$ is an almost fixed point. Then we can use linearity to get
    \begin{align*}
        \f{Var}(X) &= \E[X^2] - \E[X]^2 \\
            &= \E[(I_1 + \dots + I_n)^2] - 3^2 \\
            &= \E[(I_1^2 + \dots + I_n^2) + \sum_{k \neq j}I_kI_j] - 9 \\
            &= \sum_{i = 1}^n\E[I_i^2] + \sum_{k \neq j}\E[I_kI_j] - 9 \\
            &= n\left(\frac{3}{n}\right) + \sum_{k \neq j}\E[I_kI_j] - 9 \\
            &= \left(\sum_{k \neq j}\E[I_kI_j]\right) - 6.
    \end{align*}
    Now, to compute the expectations in the large parentheses, we must account for cases where $k$ and $j$ differ by: one index, two indices, more than two indices.
    
    (one index) There are $2n$ ways to choose $k$ and $j$ to produce this case. WLOG consider the four points 1, 2, 3, 4, where $j = 2, k = 3$. Then corresponding to points 2 and 3 we can assign 7 possible ordered pairs such that both are almost fixed points: (1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 2), (3, 4). Since there are $n(n-1)$ ways to assign any ordered pair, we have a probability of $\frac{7}{n(n-1)}$ for this case. Furthermore, the total expectation in this case would amount to
    \[
        2n \cdot \frac{7}{n(n-1)} = \frac{14}{n-1}.
    \]
    
    (two indices) Once again, there are $2n$ ways to choose $k$ and $j$ to produce this case. WLOG consider the five points 1, 2, 3, 4, 5, where $j = 2, k = 4$. Then corresponding to points 2 and 4 we can assign 8 possible ordered pairs such that both are almost fixed points: (1, 3), (1, 4), (1, 5), (2, 3), (2, 4), (2, 5), (3, 4), (3, 5). Similar to the previous case, the total expectation in this case would amount to
    \[
        2n \cdot \frac{8}{n(n-1)} = \frac{16}{n-1}.
    \]
    
    (more than two indices) Since there are $n(n-1)$ ways to pick any ordered pair of points, we subtract the previous two cases to get $n(n-1) - 4n = n^2 - 5n$ ways to choose $k$ and $j$ to produce this case. Since there is no interference between the two points, there would be $3\cdot3 = 9$ ways to assign almost fixed points. Therefore, the total expectation in this case would amount to
    \[
        (n^2 - 5n)\frac{9}{n(n-1)} = \frac{9n - 45}{n - 1}.
    \]
    
    Summing this all together, we get
    \begin{align*}
        \f{Var}(X) &= \frac{14}{n - 1} + \frac{16}{n - 1} + \frac{9n - 45}{n - 1} - 6 \\
            &= \frac{3n - 9}{n - 1} = 3 - \frac{6}{n - 1}.
    \end{align*}
    
    Now, noticing that the answers to case 3 and 4 satisfy the formula derived in case 5, we summarize:
    \[
    \f{Var}(X) = \begin{cases}
        0 & \f{ for } n = 1, 2, \\
        3 - \frac{6}{n - 1} & \f{ for } n > 2.
    \end{cases}
    \]
    
    \item \textbf{Introduction to Information Theory}
    
    (a) Using the definition, we have
    \begin{align*}
        H(X) &= -\sum_x p(x)\log p(x) \\
            &= -\sum_x \log \left(p(x)^{p(x)}\right) \\
            &= -\left(\log\left(\prod_x p(x)^{p(x)}\right)\right) \\
            &\geq 0,
    \end{align*}
    since the $0 \leq p(x) \leq 1$ for all $x$, and we have a product of terms between 0 and 1, of which the log will return a nonpositive real value, and negating gives us a nonnegative real value.
    
    (b) If $p = 1$ or $p = 0$, then the entropy would be 0. So intuitively, $p = 1/2$ should have a higher entropy than $p = 1/3$.
    
    For $p = 1/2$, we have
    \begin{align*}
        H(X) &= -\sum_{k = 0}^1 \frac{1}{2}\log_2\frac{1}{2} \\
            &= -\left(-\frac{1}{2} - \frac{1}{2}\right) \\
            &= 1.
    \end{align*}
    
    For $p = 1/3$, we have
    \begin{align*}
        H(X) &= -\left(\frac{1}{3}\log_2\frac{1}{3} + \frac{2}{3}\log_2\frac{2}{3}\right) \\
            &= -\left(\log_2\left(\left(\frac{1}{3}\right)^{\frac{1}{3}}\left(\frac{2}{3}\right)^{\frac{2}{3}}\right)\right) \\
            &= -\left(\log_2\left(\left(\frac{4}{27}\right)^{\frac{1}{3}}\right)\right) \\
            &\approx 0.9183...
    \end{align*}
    
    So the entropy when $p = 1/2$ is higher.
    
    (c) First, we see that
    \begin{align*}
        \P[Y = 0] &= \frac{1}{2}(1 - p_e) \\
        \P[Y = ~?] &= \frac{1}{2}(p_e) + \frac{1}{2}(p_e) = p_e \\
        \P[Y = 1] &= \frac{1}{2}(1 - p_e).
    \end{align*}
    
    Now, we compute the entropy, combining the cases where $Y = 0$ and $Y = 1$:
    \begin{align*}
        H(Y) &= -\left((1 - p_e)\log_2\left(\frac{1}{2}(1 - p_e)\right) + p_e\log_2p_e\right) \\
            &= -\left((1 - p_e)(-1 + \log_2(1 - p_e)) + \log_2\left(p_e^{p_e}\right)\right) \\
            &= 1 - p_e - \log_2\left((1 - p_e)^{1 - p_e}p_e^{p_e}\right).
    \end{align*}
    
    (d) First, we compute the probabilities at each point:
    \begin{align*}
        p(0, 0) &= \frac{1}{2}(1 - p_e) \\
        p(0, ?) &= \frac{1}{2}p_e \\
        p(1, 1) &= \frac{1}{2}(1 - p_e) \\
        p(1, ?) &= \frac{1}{2}p_e.
    \end{align*}
    
    Now, we compute the entropy, combining symmetric cases from above:
    \begin{align*}
        H(X, Y) &= -\left((1 - p_e)\log_2\left(\frac{1}{2}(1 - p_e)\right) + p_e\log_2\left(\frac{1}{2}p_e\right)\right) \\
            &= -\left((1 - p_e)(-1 + \log_2(1 - p_e)) + p_e(-1 + \log_2p_e)\right) \\
            &= 1 - \log_2\left((1 - p_e)^{1 - p_e}p_e^{p_e}\right).
    \end{align*}
    
    \item \textbf{Soliton Distribution}
    
    (a) The probability that a given degree $d$ packet is connected to the chunk we are about to peel off is
    \[
        \frac{\binom{N - k - 1}{d - 1}}{\binom{N - k}{d}} = \frac{\frac{(N - k - 1)!}{(d - 1)!(N - k - d)!}}{\frac{(N - k)!}{d!(N - k - d)!}} = \frac{d}{N - k}.
    \]
    Therefore, using linearity of expectation, the expected number of degree $d$ packets whose degrees will be reduced by one after removing the $(k + 1)$st packet should be 
    \[
    f_k(d) \cdot \frac{d}{N - k}.
    \]
    
    (b) For the $d = 1$ case, we have the following recurrence relation,
    \[
        f_{k + 1}(1) = f_k(2) \cdot \frac{2}{N - k},
    \]
    and if we plug in our desired $f_{k + 1}(1) = 1$ and formula $f_k(d) = (N - k)/[d(d - 1)]$, we see that the above recurrence relation holds:
    \[
        f_{k + 1}(1) = 1 = \frac{N - k}{2(1)} \cdot \frac{2}{N - k} = f_k(2) \cdot \frac{2}{N - k}.
    \]
    
    Now, for the general case where $d = 2,\dots,N$, we have the following recurrence relation,
    \[
        f_{k + 1}(d) = f_k(d + 1)\cdot\frac{d + 1}{N - k} + f_k(d)\cdot\left(1 - \frac{d}{N - k}\right),
    \]
    and if we plug in the formula $f_k(d) = (N - k)/[d(d - 1)]$, once again we see that the recurrence relation holds:
    \begin{align*}
        f_k(d + 1)\cdot\frac{d + 1}{N - k} + f_k(d)\cdot\left(1 - \frac{d}{N - k}\right) &= \frac{N - k}{d(d + 1)}\cdot\frac{d + 1}{N - k} + \frac{N - k}{d(d - 1)}\cdot\frac{N - k - d}{N - k} \\
            &= \frac{1}{d} + \frac{N - k - d}{d(d - 1)} \\
            &= \frac{N - (k + 1)}{d(d - 1)} \\
            &= f_{k + 1}(d),
    \end{align*}
    which is what we wanted. It follows that in order for $f_k(1) = 1$ to hold for $k = 0,\dots,N - 1$, we must have $f_k(d) = (N - k)/[d(d - 1)]$ for all $d = 2,\dots,N$.
    
    From this, we may deduce what $p(d)$ is for each $d > 1$. The probability of $p(d)$ for $d > 1$ satisfies $f_0(d) = N \cdot p(d)$. From the formula above, this gives us that $p(d) = \frac{1}{d(d-1)}$ for $d > 1$. For $d = 1$, we have $1 = f_1(1) = N \cdot p(1)$, which gives us $p(1) = \frac{1}{N}$.
    
    (c) We compute the expectation of $p(\cdot)$:
    \begin{align*}
        \E[p(\cdot)] &= \sum_{i = 1}^n d \cdot p(d) \\
            &= \frac{1}{n} + \sum_{i = 2}^n d \cdot \frac{1}{d(d-1)} \\
            &= \frac{1}{n} + \sum_{i = 2}^n \frac{1}{d - 1} \\
            &= \sum_{i = 1}^{n - 1}\frac{1}{i} \\
            &\approx \ln (n - 1),
    \end{align*}
    or just $\ln n$ for large $n$.
    
\end{enumerate}